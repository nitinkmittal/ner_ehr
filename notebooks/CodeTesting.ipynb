{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from collections import namedtuple\n",
    "import torch\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from torch import nn\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_ehr.training.datasets import EHRDataModule, EHRBatchCollator\n",
    "from ner_ehr.data.vocab import TokenEntityVocab\n",
    "from ner_ehr.training.models import LSTMNERTagger\n",
    "from ner_ehr.training.losses import cross_entropy\n",
    "from ner_ehr.training.metrics import accuracy_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "from pathlib import Path\n",
    "from ner_ehr.data.variables import AnnotationTuple, LongAnnotationTuple\n",
    "from ner_ehr.data.utils import df_to_namedtuples\n",
    "from ner_ehr.data.vocab import TokenEntityVocab\n",
    "from ner_ehr.data.ehr import EHR\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def read_annotatedtuples(dir: Union[str, Path]) -> List[AnnotationTuple]:\n",
    "    \"\"\"Read annotated tuples from CSVs present inside given directory.\n",
    "\n",
    "    Args:\n",
    "        dir: directory containing CSVs with annotated tokens\n",
    "\n",
    "    Returns:\n",
    "        annotatedtuples: list of AnnotatedToken tuples\n",
    "                [\n",
    "                    Annotation(\n",
    "                        doc_id='100035',\n",
    "                        token='Admission',\n",
    "                        start_idx=0,\n",
    "                        end_idx=9,\n",
    "                        entity='O'),\n",
    "                    Annotation(\n",
    "                        doc_id='100035',\n",
    "                        token='Date',\n",
    "                        start_idx=10,\n",
    "                        end_idx=14,\n",
    "                        entity='O'),\n",
    "                ]\n",
    "    \"\"\"\n",
    "    annotatedtuples = []\n",
    "    for fp in glob(os.path.join(dir, r\"*.csv\")):\n",
    "        annotatedtuples += df_to_namedtuples(\n",
    "            name=AnnotationTuple.__name__,\n",
    "            df=EHR.read_csv_tokens_with_annotations(fp),\n",
    "        )\n",
    "\n",
    "    return annotatedtuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"../processed/train_subset\"\n",
    "annotatedtuples = read_annotatedtuples(dir)\n",
    "vocab = TokenEntityVocab(to_lower=True)\n",
    "vocab.fit(annotatedtuples=annotatedtuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token_to_idx(\"Admission\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'O': 6})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token_entity_freq[\"admission\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token_entity_freq[\"Admission\"].items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict({\"a\": 6, \"b\": 6, \"c\":3}, orient=\"index\",).reset_index().sort_values(by=[0,\"index\"], ascending=[False, True]).values[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('O', 6)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocab.token_entity_freq[\"admission\"].items(), key=lambda x: (x[1], x[0]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token_to_idx(\"nitin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = EHRBatchCollator(return_meta=True)\n",
    "dm = EHRDataModule(\n",
    "    vocab=vocab,\n",
    "    collate_fn = collate_fn,\n",
    "    dir_train=\"../processed/train_subset/\",\n",
    "    # dir_val=\"../processed/val/\",\n",
    "    # dir_test=\"../processed/test/\",\n",
    "    seq_length = 128, \n",
    "    batch_size_train = 32,\n",
    "    annotated=True)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 128]), torch.Size([32, 128]), (32, 128))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = dm.train_dataloader()\n",
    "for i, (X, Y, data) in enumerate(dl):\n",
    "    if i == 0:\n",
    "        break\n",
    "X.shape, Y.shape, (len(data), len(data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy import displacy\n",
    "\n",
    "# for i, (start, end) in enumerate(para_start_indexes3):\n",
    "#     print(f\"{'='*50}{i}{'='*50}\")\n",
    "#     window = 10\n",
    "#     idx = (end - start + 2*window)//2\n",
    "    \n",
    "#     string = text[start-window:end+window]\n",
    "    \n",
    "#     ex = [{\"text\": string, \n",
    "#        \"ents\": [{\"start\": idx-1, \"end\": idx+1, \"label\": \"O\"}],\n",
    "#        \"title\": None}]\n",
    "#     html = displacy.render(ex, style=\"ent\", manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_ehr.data.embeddings import (\n",
    "    GloveEmbeddings, \n",
    "    PubMedicalEmbeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('unspecified', 0.766258955001831),\n",
       " ('seizures', 0.746199905872345),\n",
       " ('accidental', 0.7298815846443176),\n",
       " ('retaliation', 0.7024218440055847),\n",
       " ('torture', 0.6841019988059998),\n",
       " ('executions', 0.679871678352356),\n",
       " ('ordering', 0.6795619130134583),\n",
       " ('imprisonment', 0.6747194528579712),\n",
       " ('confiscation', 0.6736593842506409),\n",
       " ('deportation', 0.6688999533653259)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed = GloveEmbeddings(\n",
    "    unknown_token_embedding=np.zeros(50), \n",
    "    glove_fp=\"/scratch/mittal.nit/embeddings/glove.6B.50d.txt\")\n",
    "embed.load_word2vec()\n",
    "embed.embeddings.most_similar(\"seizure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch: 1/1, batch=0, loss=2.89801: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s]\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 50\n",
    "NUM_CLASSES = vocab.num_uniq_entities\n",
    "VOCAB_SIZE = vocab.num_uniq_tokens\n",
    "HIDDEN_SIZE = 32\n",
    "EPOCHS = 1\n",
    "\n",
    "embedding_weights = np.zeros((vocab.num_uniq_tokens, EMBEDDING_DIM), dtype=np.float)\n",
    "for token, idx in tqdm(vocab._token_to_idx.items(), leave=False, position=0):\n",
    "    embedding_weights[idx] = embed(tokens=token)[0]\n",
    "embedding_weights = torch.tensor(embedding_weights, dtype=torch.float32) \n",
    "\n",
    "lstm = LSTMNERTagger(\n",
    "    embedding_dim=EMBEDDING_DIM, \n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE, \n",
    "    num_classes=NUM_CLASSES, \n",
    "    # embedding_weights=embedding_weights, \n",
    "    bidirectional=True)\n",
    "\n",
    "dl = dm.train_dataloader()\n",
    "adam = Adam(lstm.parameters(), lr=.001)\n",
    "t = tqdm(range(EPOCHS))\n",
    "losses = [] \n",
    "for i in t:\n",
    "    for j, (X, Y, data) in enumerate(dl):\n",
    "        Y_hat = lstm(X)\n",
    "        loss = cross_entropy(Y_hat, Y)\n",
    "        losses.append(loss.item())\n",
    "        adam.zero_grad()\n",
    "        loss.backward()\n",
    "        adam.step()\n",
    "        t.set_description(f\"epoch: {i+1}/{EPOCHS}, batch={j}, loss={losses[-1]:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 128, 17]), torch.Size([32, 128]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.eval()\n",
    "with torch.no_grad():\n",
    "    Y_hat = lstm(X)\n",
    "# Y_hat = torch.argmax(Y_hat, axis=-1)\n",
    "Y_hat.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2697, 0.3675, 0.1870, 0.0703, 0.1055],\n",
       "        [0.0492, 0.4234, 0.0826, 0.4139, 0.0310],\n",
       "        [0.1586, 0.2004, 0.2256, 0.1752, 0.2403],\n",
       "        [0.1779, 0.3017, 0.2987, 0.1736, 0.0480],\n",
       "        [0.1276, 0.2928, 0.1792, 0.3584, 0.0420]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_hat = torch.randn((5,5), requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    X = nn.functional.softmax(Y_hat, dim=-1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01037577, 0.11494253, 0.03125   , 0.        , 0.16666667,\n",
       "       0.        , 0.        , 0.07142857, 0.        , 0.        ,\n",
       "       0.07692308, 0.        , 0.        , 0.35      , 0.10416667,\n",
       "       0.28571429, 0.        ])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(Y_hat, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed = PubMedicalEmbeddings(\n",
    "#     unknown_token_embedding=np.zeros(200), \n",
    "#     pubmed_fp=\"/scratch/mittal.nit/embeddings/pubmed2018_w2v_200D/pubmed2018_w2v_200D.bin\")\n",
    "# embed.load_word2vec()\n",
    "# embed.embeddings.most_similar(positive=\"seizure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "214"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_not_in_embed = len(\n",
    "    [token for token in vocab._token_to_idx.keys() if token.lower() not in embed.embeddings.key_to_index.keys()])\n",
    "num_unknown_embed = 0\n",
    "for embedding in embeddings:\n",
    "    if np.allclose(embedding, embed.unknown_token_embedding):\n",
    "        num_unknown_embed+=1\n",
    "assert num_tokens_not_in_embed == num_unknown_embed\n",
    "num_tokens_not_in_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ner_ehr.utils import save_np, load_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_np(embeddings, fp=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeddings = load_np(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(embeddings, new_embeddings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner_ehr",
   "language": "python",
   "name": "ner_ehr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
